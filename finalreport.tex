\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{sectsty}
\usepackage{indentfirst}
\usepackage[super]{nth}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{caption}
\usepackage{subcaption}

\title{An Analysis of the Applications of Computer Vision on Identifying Lost Dogs}
\author{Aidan Vickars, Anant Sunilam Awasthy, Karthik Srinatha, and Rishabh Kausha}
\date{\today}

\begin{document}

\maketitle

\newpage
\section{Motivation and Background}
As the most common house pet by a large margin, lost dogs are a frequent problem around the world.  With this is mind, it stands to reason their is no shortage of interest in new techniques for finding a lost dog.  Of course, there are a variety classical of methods that include posting flyers on telephone poles, posting adds on Craiglist and social media sites as well as leveraging web applications such as the "BC SPCA Pet Search" that run by the British Columbia Society for the Prevention of Cruelty to Animals (BC SPCA).  However, all of these methods require creating some form of an eye catching poster or description that have been used in so many different forms that they have lost their intended affect.  As a result, a new method is needed.  Thus, in this paper we will present a data pipeline implemented as an Android Application that leverages three separate convolutional neural networks to match lost dogs with found dogs and vice versa by computing the similarity between lost and found dogs and subsequently returning the most similar results to the user.

To measure the success of our pipeline we devised a test by randomly designating 1000 dogs from the test portion of our custom data-set as "lost", and randomly designating 100 of these "lost" dogs as "found", and subsequently applied our data pipeline on these dogs.  We found that using our current hyper-parameters, our model matched the "lost" dogs with the "found" 100\% of the time (ADD REAL STAT HERE).  Our data collecting, processing procedures, models are all presented, and the source code has been made publicly available.

\section{Related Work}
	While dog-identification is a sub-class of the heavily researched facial recognition area, dog-identification remains extremely undeveloped.  However, there are two related works that we discuss here.  The first is "A Deep Learning Approach for Dog Face Verification and Recognition" (CITATION)  by  Guillaume Mougeit, Dewei Li and Shuai Jia.  In "A Deep Learning Approach for Dog Face Verification and Recognition", Mougeit, Li and Jia present "VGG-like and "ResNet-like" (page 422) models to encode the image of a dog to $\mathbb{R}^n$ and compute the Euclidean distance between the encoding of two images to produce measure of the similarity between the images of two dogs.  This probability is used to perform face verification to determine if two dogs are the same.  To quantify the accuracy of their models they generated 2500 positive pairs and 2500 negative pairs images of various dog faces.  In this scenario positive indicates the images represent the same dog and negative indicates the images represent different dogs.  Their models made the correct decision 92\% and 91\% of the time for each model respectively.

	The second work is "Dog Identification using Soft Biometrics and Neural Networks" (CITATION) by Kenneth Lai, Xinyuan Tu and Svetlana Yanushkevich.  In this paper, Lai, Tu and Yanushkevich present an approach to increase the accuracy in dog-identification that is the inspiration behind the work presented here, and all credit is given to them with respect to the similarities between our works.  Lai, Tu and Yanushkevich developed a dog detection model, a breed classification model, and a dog-identification model that work together in the order specified.   The dog detection model determines the bounding box of the face of a dog and can be used to crop the image accordingly.  The breed classification model like other models of this type, simply determines the most likely breed of the dog.  Finally, the dog-identification model functions in the same way as the models created  Mougeit, Li and Jia (CITATION) presented above.  By first cropping the image, and determining the breed of the dog, the number of comparisons that are made using the dog-identification model is significantly reduced and improves the face verification accuracy.  However, we do not list accuracy of the models as the dog-identification model is trained on the "Flickr-dog" dataset that contains only 374 images made up of just two breeds.  As a result, our findings are incomparable.

\section{Problem Statement}
	It should be noted that in both papers discussed above, the dog-identification models only use the face of a dog.  That is the entire body of the dog is cropped out and only the face is given to the model.  This presents a problem or deficiency in two ways.  The first is that by training the model on a data set that appears to be highly curated and contains only front facing dog faces, the crucial assumption is made that all applicable images of dogs all contain the dog's face in a relatively front facing fashion.  This is obviously not the case.  As any dog owner knows, convincing your dog two look at the camera is a non-trivial task and that the majority of their photos are of the dog in an appealing position with their face obscured.  An example of this is shown in Figure \ref{fig:x dog no face} below.  


\begin{figure}[h]
\centering
	\includegraphics{final-report-images/nofacedog.jpg}
\caption{An Example of a Dog with their Face Obscured}
\label{fig:x dog no face}
\end{figure}
The second, is the by cropping the image to only the dogs face we theorize that valuable information is lost.  In the case of face verification in humans, only examining the face is desirable because humans change clothes.  However, dogs do not.  We certainly note that there are cases where dogs do wear clothes but these cases are infrequent.  Thus, we theorize that by leveraging a dogs entire dog we may see an improvement in the accuracy of the dog-identification model relative to work done by  Lai, Tu and Yanushkevich because the model could leverage additional characteristics such as the size of the dog.  This is illustrated in Figures \ref{fig:similar faces} and \ref{fig:different bodies} where in Figure 2 we see two dogs with similar faces.  One could forgive a model for classifying these dogs as the same.  But as shown in Figure 3 we can clearly see that are not.  By leveraging the entire body of dog, this miss-classification should be eliminated.

\begin{figure}[h]
\centering
	\includegraphics{final-report-images/similar_faces.png}
\caption{Sample Images of Two Dogs with Similar Faces}
\label{fig:x similar faces}
\end{figure}

\newpage

\begin{figure}[h]
\centering
	\includegraphics{final-report-images/different_bodies.png}
\caption{Sample Images of Two Dogs with Different Bodies}
\label{fig:x different bodies}
\end{figure}

Thus, we can now present the key problems this project aims to answer:

\begin{enumerate}
  \item By leveraging the entire body of a dog, can we construct an dog-identification model that can accurately determine if two dogs are the same or not?  Furthermore, by incorporating the entire body of a dog, can we achieve a better accuracy than that achieved by Mougeit, Li and Jia?
  \item By removing the restriction of curated front facing dogs, can we construct a pipeline that can accurately match lost dogs with found dogs and vice versa? 
\end{enumerate}

\section{Data Product}

	To answer the questions stated above, we use the work done by Mougeit, Li and Jia and use a similar VGG model to compare dogs and we also incorporate the work done by Lai, Tu and Yanushkevich and create a dog localization model and breed classification model to improve accuracy.   We further extend this by using the entire body of a dog instead of the face with the intention to improve accuracy.  In future references we refer to the these models as the Dog Comparator, the Dog Extractor and the Dog Classifier respectively.  However, before elaborating on each individual model, we will first present the Data Product to give the reader context as to how the models work together.

	To utilize this work in a production environment, we have created an android application (app) to act as user interface to allow for easy image upload, and have packaged the models into a Flask API (API) to match lost and found dogs. We also use an AWS S3 bucket and a Relational Database System (RDS) to store images and image metadata respectively.  This system is visualized below in Figure \ref{fig:x app system}.

\begin{figure}[h]
\centering
	\includegraphics[scale=0.1]{final-report-images/system.jpeg}
\caption{Dog Finder System}
\label{fig:x app system}
\end{figure}

At a lower level if the user has lost a dog, they will submit a photo and as well as their contact information and location via the app.  The app will then submit this information to the API.  Once a submission has been made to the API, the dog finder pipeline that is visualized below in Figure: \ref{fig:x app pipeline} is triggered.  The following steps outline this pipeline:
\newpage

\begin{figure}[h]
\centering
	\includegraphics[width=1.0\textwidth]{final-report-images/applowlevel.png}
\caption{Dog Finder Pipeline}
\label{fig:x app pipeline}
\end{figure}

\begin{enumerate}
  
  \item Once a lost dog has been submitted, the image is passed to the "Dog Extractor" model that computes the coordinates of the bounding box of the dog.  This model also acts as a quality control by validating the image to ensure that the image contains a dog, and contains only one dog.  If these conditions are not met, an error is returned to the user.
  
  \item After validation, the original image is saved into an S3 bucket, and the related information such as the users contact information, location and the coordinates of the bounding box are inserted into a PostGre relational database system (RDS).
  
  \item The original image is then cropped using the computed bounding box coordinates, and passed into the "Dog Classifier" model that computes the top $k$ most likely breeds.  This information is inserted into the RDS.
  
  \item The cropped image is also passed into the Dog Comparator model that creates a five dimensional encoding of the image.
  
  \item We then query the encodings of the dogs marked as found according to breed and location, and compare against the lost dog by computing the euclidean distance between the encodings of the lost dog and the found dogs.  The sigmoid function is applied to the distance value to constrain the it fall within $[0,1]$ or in other words, a probability.  The $n$ most similar dogs are returned to the user.
  
  \item If a match is confirmed by the user, the corresponding lost and found dogs are removed from the RDS and S3 bucket.  Otherwise, the lost dog is left in the system for future comparisons.
  
\end{enumerate}



An attentive reader will notice that we discuss only the submission of lost dogs.  This is done to minimize confusion.  If the user submits a found dog the pipeline is identical except that the dog is instead marked as found and is compared against dogs marked as lost.  This completes the pipeline contained within the application.  However, each model will be discussed in detail individually in the sections below.

\section{Data Science Pipeline}

Now that the Data Product has been adequately explained, we can delve into each model.  We being first with the Data Science Pipeline to describe the data used by each model respectively.  For the Dog Extractor model, we utilized the "Open Images" (SOURCE) data-set that contains thousands of images of dogs with corresponding bounding boxes.  While this data is already relatively clean, we discarded all grey-scale images, and converted all images to RGB format.  The decision was made to discard grey-scale images because our expectation is that in the production environment of our Data Product, the vast majority of images will be colour images.  After cleaning this left 19 995 training images, 1568 validation images , and 4791 test images.

For the Dog Classifier Model ---------------------

Now, to train the Dog Comparator model we required multiple pictures of many individual dogs where each picture contained the entire body of the dog.  However, we found that there was no data-set that met these requirements.  We note that the "Stanford Dog Data-set" exists that contains multiple images of 1425 individual dogs.  However, each image contained only the face of a dog and the images appeared to be highly curated.  To be succinct the majority of the images were front facing in similar positions which as discussed above is undesirable.  To solve this we scraped the trove of images on Petfinder.com that at the time of writing lists over 100 000 dogs for adoption across the world where almost every dog has multiple images.  However, scraping the images presented a challenge because the links to every dog are dynamically generated.  This meant scraping the HTML of the web page containing the grid of available dogs using Python's request package was not sufficient because during download the URLs pointing to each available dog would not be included due to their dynamic creation.  To solve this, we split the scraping into two parts.  We first created a Selenium application in Python that scraped the URLs pointing to each individual dog.  Then, using these URLs we scraped and downloaded the images for every dog and also recorded additional information such as name, breed, age, size etc.  This resulted in images for 9729 dogs with 0 - 6 images for every dog.  Once the data was downloaded, we applied the following cleaning process on the images of every dog:
\begin{enumerate}
  
  \item If the dog has 1 or a less images, we discarded the dog and its images
  
  \item Confirmed every image was in RGB format or convert it to RGB format.  Otherwise the image was discarded.
  
  \item Passed every image into the Dog Extractor Model:
    \begin{itemize}
      \item Verified the image contained a dog
      \item Verified the image contained only one dog
      \item Recorded the bounding box coordinates of the dog
    \end{itemize}
    If either of the conditions in the first two bullets were not met, the image was discarded.
    
  \item If after the previous step, the Dog had 1 or less images we discarded the dog and its images.
  
\end{enumerate}

\noindent After cleaning we were left with 8349 dogs with 2 - 6 images for every dog.  The data set was then divided into a Train, Validation and Test split of 70\%, 10\%, and 20\% respectively.  This gave a total of 6679 training dogs, 501 validation dogs and 1169 testing dogs respectively.  We do note that we did not account for the actual number of dog images contained in each split.  This is because during the training and testing process we only considered the images on a dog by dog bases.  This will become more clear in the presentation of the Dog Comparator model below.

\section{Methodology}
Now that the data-sets used have been outlined, the approaches used in each model can be discussed.  For all three models, we present the architecture used in and their respective results.  We also perform a deep analysis into their strength and weaknesses.  

\subsection{Dog Extractor}

To develop the Dog Extractor model, we investigated multiple approaches that included developing an original implementation of a transfer learning approach to Yolo V2 (CITATION).  However, we found a significant limiting factor to be a lack of GPU memory.  To be succinct, we trained the Dog Extractor model on an RTX 3070 with only 8 GB of memory.  Because we wanted to use larger and more complex models to achieve high degrees of accuracy, our models would train very slowly due to the requirement of having a very small batch size during training because of memory limitations.  This necessitated the requirement to utilize a largely pre-trained model via transfer learning and only make small adjustments with minimal amounts of additional training.  To achieve this we employed transfer learning using a pre-trained Faster RCNN (CITATION) model using PyTorch with a feature extractor trained on the ResNet 50 data-set (CITATION) to act as the back bone of the network.  We adjusted the output of the model from predicting a multitude of classes to only two so that it acted as an dog localization model.  To be succinct the model was adjusted to predict only the background class and the dog class.  We then trained the model on the cleaned "Open Images" data-set for 10 additional epochs with an initial learning rate of 0.005 and a decay of 0.1 every 3 epochs, as well as a momentum value of 0.9.  Due to memory limitations, our batch size was set to one.  We note that PyTorch's tutorial on object detection (CITATION) was very helpful here and we give all credit accordingly.  During training we concerned ourselves only with the validation Mean Average Precision because we are concerned only with a single class.  We coded and computed the MAP over an IOU threshold range from 0.5 to 0.95 in increments of 0.05, and computed the mean.  We denote this value as MAP 0.5:0.95.  During training we saved the model weights only when the MAP 0.5:0.95 increased and achieved a best validation MAP 0.5:0.95 of 0.73 during training using an object score threshold of 0.6 and an IOU threshold of 0.5 in non max suppression (NMS).  The MAP 0.5:0.95 is plotted below in Figure \ref{fig:x epoch_v_map} over 10 epochs. 

\begin{figure}[h]
\centering
	\includegraphics[scale=0.7]{final-report-images/epoch_v_map.png}
\caption{Epoch vs. MAP 0.5:0.95}
\label{fig:x epoch_v_map}
\end{figure}

\newpage

Note if the reader is unfamiliar with NMS we outline the algorithm below:  \\

\begin{minipage}{1\textwidth}%
  \noindent \textbf{Input:} List of bounding box proposals and object score pairs.  Each pair gives the coordinates of box surrounding the object and the confidence that the bounding box contains a dog respectively. \\
  
  \noindent \textbf{Output:} Filtered list of bounding box proposals and object score pairs \\
  
  \noindent \textbf{Algorithm:} \\
\end{minipage}%

\begin{enumerate}

  \item Initialize empty list, lets call it list B.

  \item Order the bounding boxes and object score pairs according to the object score in descending order.
  
  \item Discard any pairs whose object score is less than the pre-defined object threshold.  Bounding boxes with an object score lower than this threshold likely do not bound anything.
  
  \item While there are still bounding boxes and object score pairs on the list:
        \begin{itemize}
             \item Pop the first pair off the list and record it in list B.
            
             \item Compute the intersection over union (IOU) between the pair just popped off the list, and all remaining pairs on the list.
             \item If the resulting IOUs are greater than the predefined IOU threshold, discard the corresponding pairs.  The bounding boxes of these pairs likely bound the same dog.
        \end{itemize}
  

  
\end{enumerate}

To improve the accuracy of the model we tune the object score and IOU thresholds used in NMS.  To do this we used a brute force approach by applying NMS on the model output on the validation data over a grid of object score and IOU thresholds and then compute MAP 0.5:0.95 on the results.  We use a large grid of that ranges from 0.05 to 0.95 for both thresholds.  The results are visualized below in Figure \ref{fig:x object v iou}.  

\begin{figure}[h]
\centering
	\includegraphics[scale=0.7]{final-report-images/map0.5to0.95.png}
\caption{Object vs. IOU Threshold MAP 0.5:0.95}
\label{fig:x object v iou}
\end{figure}

\noindent We achieve the maximum MAP 0.5:0.95 of 0.75 in the top right corner of the plot using an object threshold of 0.05, and an IOU threshold of 0.9.  However, we consider the consequences of using such extreme thresholds.  By using such a low object threshold, almost any bounding box the model produces will be include regardless of how strongly or rather how uncertain the model is that there is a dog in the bounding box.  This will increase the number of false positives the model produces.  In a similar fashion, because using a strong IOU threshold of 0.9, any pair of bounding boxes must achieve an IOU greater than 0.9 to classified as bounding the same dog.  This will again increase the false positive rate.  To combat this, we deviate from the optimum thresholds, and increase the object threshold to 0.5 and decrease the IOU threshold to 0.75.  This combination achieves an MAP 0.5:0.95 of 0.74.  A very small decrease of just 0.01.  Finally, using our chosen thresholds, the model achieves an MAP 0.5:0.95 of 0.74 on the test data indicating the model performs very well.



\subsection{Dog Classifier}

\subsection{Dog Comparator}
To create the dog comparator model that assesses the similarity between two dogs, we leveraged the pre-trained feature extractor from the VGG 19 classifier (CITATION) and added three additional layers.  This is visualized in Figure \ref{fig:x comparator} below.

\begin{figure}[h]
\centering
	\includegraphics[scale=0.4]{final-report-images/dog_comparator.png}
\caption{Dog Comparator Model Architecture}
\label{fig:x comparator}
\end{figure}

\noindent An attentive reader will notice the large decrease in dimension in the first dense layer.  This was done to accommodate GPU memory limitations so that the batch size could be increased during training.   

Now, with respect to training we used the following process to generate a single batch element: \\

\begin{minipage}{1\textwidth}%
  \noindent \textbf{Input:} Index of a dog in the data-set \\
  
  \noindent \textbf{Output:} Positive Image, Negative Image, and Anchor Image \\
  
  \noindent \textbf{Algorithm:} \\
\end{minipage}%

\begin{enumerate}

  \item Randomly choose two different images of the indexed dog.  Assign one image as the positive image and anchor image respectively. 

  \item Randomly select a different dog, and randomly select an image.  Assign this image as the negative image.
  
  \item Return the positive, anchor and negative images.

\end{enumerate}

Each image was then passed through the model and the triplet loss was computed that can be defined as $L(P, A, N) = ReLU(\sigma(Dist(P, A)) - \sigma(Dist(P, N)) + M)$ where $P, A$ and $N$ are the positive, anchor and negative images respectively, $Dist()$ is the euclidean distance between their encodings and $M$ is the margin that was set to 0.9.  The model was trained for 41 epochs with a learning rate of 0.01 that decayed by 0.1 every four epochs.  The training and validation losses are vizualized below in Figure \ref{fig:x epoch_v_map} 

\newpage

\begin{figure}[h]
\centering
	\includegraphics[scale=0.7]{final-report-images/triplet_training.png}
\caption{Dog Comparator Model Training Using a Triplet Loss Function}
\label{fig:x epoch_v_map}
\end{figure}
Now, since the model simply encodes the image into a five dimensional space it should be noted how the model is used assess the similarity between two dogs.  This is done by computing the euclidean distance between the encodings of two images and then applying the sigmoid function to the computed value.  Lets call this the similarity value.  If two dogs are very similar, their encodings should be relatively close together in the five dimensional space.  As a result, the distance between them and the similarity value should be close to zero.  In contrast, if two dogs are very dissimilar the distance between them should be large and the resulting similarity value should be close to one.  However, because we did not constrain the training data to only front facing dog faces all images even of the same dogs are generally dissimilar.  As a result, we found that similar dogs tended to have a similarity value near or slightly above 0.5 while dissimilar dogs have a normal similarity value near one.  This is reflected in the optimum classification threshold discussed below.

To assess how well the dog comparator model works independently, we first computed the optimum classification threshold by computing the approximate $knee$ of the ROC curve on the validation data to minimize the false positive rate and maximize the true positive rate.  The ROC curve is vizualized below in Figure \ref{fig:x val roc curve}.  We chose a classification threshold of 0.79.

\begin{figure}[h]
\centering
	\includegraphics[scale=0.7]{final-report-images/roc_curve_validation_triplet.png}
\caption{Validation ROC Curve Using a Triplet Loss Function}
\label{fig:x val roc curve}
\end{figure}

\noindent This resulted in a strong test accuracy and F1 score of approximately 0.87 respectively.  However, we noticed that after examining the similarity values of the model on the validation data, we found it did not do well in dividing the comparisons that contained the same dog and comparisons that contained different dogs.  This is vizualized below in the line plot contained in Figure \ref{fig:x triplet lineplot} where we can see fairly significantly overlap between both groups.

\begin{figure}[h]
\centering
	\includegraphics[scale=0.7]{final-report-images/triplet_lineplot.png}
\caption{Line Plot of Similarity Scores Using a Triplet Loss Function}
\label{fig:x triplet lineplot}
\end{figure}
To remedy this, we retrained our model for 49 epochs with the same learning rates as above using a cross entropy loss, and adjusted the individual batch elements to randomly be either be two images of the same dog, or two images of different dogs.  Using the same methodology, we determined the optimum classification threshold to be 0.67.  This resulted in a modest increase in test accuracy and F1 score to 0.89 for each respectively.  This improvement can be seen in the line plot of the similarity scores of the new model applied on the validation data that is shown in Figure \ref{fig:x triplet lineplot}.  We can clearly see an improved divide between the two groups.

\begin{figure}[h]
\centering
	\includegraphics[scale=0.7]{final-report-images/crossentropy_lineplot.png}
\caption{Line Plot of Similarity Scores Using a Cross Entropy Loss Function}
\label{fig:x triplet lineplot}
\end{figure}
The accuracy of $89\%$ is highly comparable compared to models created by Mougeit, Li and Jia that had an accuracy $91\%$ and $92\%$.  However, we note that while our model features a $3\%$ decrease in accuracy, we allow for a far more diverse set of images by not applying our model solely on front facing images of dog faces.

However, an observant read will note that by using images from two randomly selected dog during both the training and testing processes differing between dogs is an easy task for the model.  This is because dogs of different breeds tend to be obviously dissimilar and as a result telling them apart is easy.  To account for this, we perform additional testing of our model by testing the performance of the model over different dogs that are known to be similar.

To do this we further cleaned the data from Petfinder.com and parsed the available breeds.  To be succinct, the breed was available for most dogs in the data-set but it could be in many forms such as "German shepherd dog" and "terrier" where capitalization was not consistent and inconsequential words like "dog" were contained in the breed.  Furthermore, the data-set contained mixed breeds that were in the form of "German shepherd & terrier".  This meant that the breeds needed to be standardized.  We did this by first converting all strings to lowercase, removing useless words such as "mixed", "breed", and "dog" and splitting the breed into two strings using '&' to account for mixed breeds.  We then compared all breeds contained in our data-set to the standardized list of 120 breeds contained in the "Stanford Dogs Data-set" (CITATION) by computing the cross product between them and computing the Jaccard similarity between the two.  We then filtered out any pairs that had a similarity less than 0.75.  By doing this, we successfully standardized the breeds contained in the data-set.  Using the standardized breeds, we adjusted the generation of a single batch element during training and testing by modifying the case when two images of different dogs are produced to produce two different dogs of the same breed.  Thus producing two similar dogs.  The model was then tested on the augmented test data, and achieved a significantly decreased but still respectable classification accuracy and F1 score of 0.79 and 0.77 respectively.   To attempt to improve this we trained the model for an additional four epochs on the augmented training data and saw no improvement.  
    
Upon further inspection, we found a crucial flaw in the data.  The data is biased towards the most popular dog breeds.  Popular Dog breeds such as Labrador Retrievers and Chihuahuas are over-represented in the data, while less popular breeds like Saint Bernards and English Setters are underrepresented.  We theorize that by performing additional training on the augmented training data we only further biased the model towards these dogs.  Unfortunately, we are not able to account for this flaw in the data.  This is because in other data-sets, one could augment the data by adding random samples from the underrepresented groups with an additional degree of random variation as well.  However, in this case this we are unable to do this without creating two significant issues.  The first, is that by adding random samples of the underrepresented breeds we risk over-fitting the model to specific dogs.  We suspect this would occur because some of the underrepresented breeds have only a few instances and thus adding random samples of these breeds would require randomly sampling the same dogs many times.  The second, is by adding some degree of variation to the images of the randomly sampled dogs the images would likely be distorted significantly.  It should be noted that there is a possible solution to this.  We theorize that unique images of underrepresented breeds could be generated using a generative adversarial network.  However, this is outside the scope of this project.

\section{Methodology}

\section{Lessons Learn}

\section{Summary}















\end{document}
