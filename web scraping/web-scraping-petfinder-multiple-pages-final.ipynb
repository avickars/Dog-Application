{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82df968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code\n",
    "# ref: https://www.dataquest.io/blog/web-scraping-beautifulsoup/ \n",
    "# ref: https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/ \n",
    "    \n",
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request # to download images\n",
    "import re\n",
    "import os # to make a directory\n",
    "import csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e746a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a93342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3e9d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://stackoverflow.com/questions/2081836/how-to-read-specific-lines-from-a-file-by-line-number\n",
    "\n",
    "f = open(\"pet_links.txt\", \"r\")\n",
    "\n",
    "for line_count, line in enumerate(f):\n",
    "    \n",
    "    if(line_count == 1000):\n",
    "        break\n",
    "        \n",
    "    # line_count starts from 0, so 45th line would be when line_count = 44 \n",
    "    elif line_count >= 44:\n",
    "        \n",
    "        print(\"\\n dog link details (line_count = \", line_count, \"): \\n\")\n",
    "        print(line, \"\\n\")\n",
    "\n",
    "        # strip \\n from the end of the link\n",
    "        link = line.rstrip('\\n')\n",
    "\n",
    "        page = requests.get(link)\n",
    "        \n",
    "        # if dog page is no longer available (404 or 500), continue to next doge page link\n",
    "        if((page.status_code//100 == 4) or (page.status_code//100 == 5)):\n",
    "            print(\"dog page no longer available, skip it \\n\")\n",
    "            continue\n",
    "            \n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        dog_name = soup.find(attrs={\"data-test\" : \"Pet_Name\"})\n",
    "        dog_name = re.sub('\\s+', '', dog_name.contents[0]).lower() # remove \\n, tabs and spaces from string\n",
    "\n",
    "        images = []\n",
    "        for image in soup.find_all('img'):\n",
    "            images.append(image['src'])\n",
    "\n",
    "        substring = '/'\n",
    "        img_counter = 1\n",
    "        unique_images = []\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            substring = substring + str(img_counter) + \"/\" \n",
    "            if(substring in images[i]):\n",
    "                unique_images.append(images[i])\n",
    "            substring = '/'\n",
    "            img_counter = img_counter + 1\n",
    "\n",
    "\n",
    "        unique_dog_dir = ''\n",
    "        unique_dog_dir = unique_dog_dir + dog_name\n",
    "\n",
    "        dog_breed = soup.find(attrs={\"data-test\" : \"Pet_Breeds\"}).text.strip()\n",
    "\n",
    "        unique_dog_dir = unique_dog_dir + '-' + dog_breed.lower().replace(\" \", \"-\") # breed to dogname\n",
    "\n",
    "        pet_location = soup.find(attrs={\"data-test\" : \"Pet_Location\"}).text.strip()\n",
    "\n",
    "        dog_location =  pet_location.split(\",\")\n",
    "        dog_city = dog_location[0]\n",
    "        dog_state = dog_location[1]\n",
    "\n",
    "        pet_location_dir = re.sub('\\s+', '', pet_location).lower() # remove \\n, tabs and spaces from string\n",
    "\n",
    "        unique_dog_dir = unique_dog_dir + '-' + pet_location_dir.replace(\",\", \"-\") # location to dogname\n",
    "\n",
    "        dog_location =  pet_location.split(\",\")\n",
    "        dog_city = dog_location[0]\n",
    "        dog_state = dog_location[1]\n",
    "\n",
    "        dog_age = soup.find(attrs={\"data-test\" : \"Pet_Age\"}).text.strip()\n",
    "        dog_sex = soup.find(attrs={\"data-test\" : \"Pet_Sex\"}).text.strip()\n",
    "        dog_size = soup.find(attrs={\"data-test\" : \"Pet_Full_Grown_Size\"}).text.strip()\n",
    "\n",
    "\n",
    "        print(\"dog_name: \", dog_name)\n",
    "        print(\"dog_breed: \", dog_breed)\n",
    "\n",
    "        print(\"dog_city: \", dog_city)\n",
    "        print(\"dog_state: \", dog_state)\n",
    "        print(\"unique_dog_dir: \", unique_dog_dir)\n",
    "\n",
    "        print(\"dog_age: \", dog_age)\n",
    "        print(\"dog_sex: \", dog_sex)\n",
    "        print(\"dog_size: \", dog_size)\n",
    "\n",
    "        unique_directory_counter = 1\n",
    "\n",
    "        try:\n",
    "            path = os.path.join(\"/Volumes/RK - T7/bd2_project_data/images/\", unique_dog_dir)\n",
    "            os.makedirs(path)\n",
    "        except FileExistsError:\n",
    "            # directory already exists - append unique num at the end of dir to make it unique\n",
    "            path = os.path.join(\"/Volumes/RK - T7/bd2_project_data/images/\", unique_dog_dir + '-' + str(unique_directory_counter))\n",
    "            os.makedirs(path)\n",
    "            unique_directory_counter = unique_directory_counter + 1\n",
    "            pass\n",
    "\n",
    "        # download dog's images in that unique directory\n",
    "        unique_images_counter = 1\n",
    "        for image_url in unique_images:\n",
    "            # urllib.request.urlretrieve(image_url, \"try_images/try.jpg\")\n",
    "            dog_image_name = dog_name + '-' + str(unique_images_counter) + \".jpg\"\n",
    "            dog_image_path = path + '/' + dog_image_name\n",
    "\n",
    "            urllib.request.urlretrieve(image_url, dog_image_path)\n",
    "\n",
    "            unique_images_counter = unique_images_counter + 1\n",
    "            print(image_url)\n",
    "            print(dog_image_path)\n",
    "\n",
    "        #ref: https://www.pythontutorial.net/python-basics/python-write-csv-file/\n",
    "\n",
    "        header = ['dog_name', 'dog_breed', 'dog_city', 'dog_state', 'unique_dog_dir', 'dog_age', 'dog_sex', 'dog_size', 'dog_images']\n",
    "        data = [dog_name, dog_breed, dog_city, dog_state, unique_dog_dir, dog_age, dog_sex, dog_size, len(unique_images)]\n",
    "\n",
    "        csv_file_name = dog_name + '.csv'\n",
    "\n",
    "        with open(path + '/' + csv_file_name, 'w', encoding='UTF8') as f:\n",
    "            writer = csv.writer(f)\n",
    "\n",
    "            # write the header\n",
    "            writer.writerow(header)\n",
    "\n",
    "            # write the data\n",
    "            writer.writerow(data)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325643d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
