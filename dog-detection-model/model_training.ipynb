{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d1f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "from data_loader import OpenImagesDataset\n",
    "from model_utils import device\n",
    "from model_transformations import Transformations\n",
    "from torch.utils.data import DataLoader\n",
    "from params import NUM_EPOCHS, IMAGE_SIZE, GRID_SIZE, NUM_ANCHOR_BOXES\n",
    "import torch.optim as optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3ce582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the training data\n",
    "trainingData = OpenImagesDataset(rootDirectory='open-images-v6', \n",
    "                                 anchorBoxes='centroids.npy', \n",
    "                                 transform=Transformations, \n",
    "                                 dataType='validation', \n",
    "                                 gridSize=GRID_SIZE, \n",
    "                                 imageSize=IMAGE_SIZE)    \n",
    "\n",
    "# Defining the training data\n",
    "trainDataLoader = DataLoader(dataset=trainingData, \n",
    "                             batch_size=2,\n",
    "                             num_workers=1,\n",
    "                             shuffle=True)\n",
    "\n",
    "# Reading in the training data\n",
    "validationData = OpenImagesDataset(rootDirectory='open-images-v6', \n",
    "                                 anchorBoxes='centroids.npy', \n",
    "                                 transform=Transformations, \n",
    "                                 dataType='validation', \n",
    "                                 gridSize=GRID_SIZE, \n",
    "                                 imageSize=IMAGE_SIZE)    \n",
    "\n",
    "# Defining the training data\n",
    "validationDataLoader = DataLoader(dataset=trainingData, \n",
    "                             batch_size=2,\n",
    "                             num_workers=1,\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f99e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def space_to_depth(x, block_size):\n",
    "    # CITATION: https://stackoverflow.com/questions/58857720/is-there-an-equivalent-pytorch-function-for-tf-nn-space-to-depth\n",
    "    n, c, h, w = x.size()\n",
    "    unfolded_x = torch.nn.functional.unfold(x, block_size, stride=block_size)\n",
    "    return unfolded_x.view(n, c * block_size ** 2, h // block_size, w // block_size)\n",
    "\n",
    "class DogDetectorModel(nn.Module):\n",
    "    def __init__(self, modelPath=None, gridSize=13, numAnchorBoxes=7):\n",
    "        super(DogDetectorModel, self).__init__()\n",
    "        \n",
    "        # Recording the grid size\n",
    "        self.gridSize = gridSize\n",
    "        \n",
    "        # Recording the number of anchor boxes\n",
    "        self.numAnchorBoxes = numAnchorBoxes\n",
    "        \n",
    "        # Reading in the pre-trained feature extractor\n",
    "        self.featureExtractor = models.vgg19_bn(pretrained=True).features\n",
    "        \n",
    "        # Freezing the weights of the pre-trained feature extractor\n",
    "        for parameter in self.featureExtractor.parameters():\n",
    "            parameter.requires_grad = False\n",
    "            \n",
    "        # Defining the object detection layers\n",
    "        # These layers follow right up to the line in Table 6 (Darknet-19) in YoloV2 Paper\n",
    "        self.objectDetectorPart1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.LeakyReLU(0.1),        \n",
    "            \n",
    "            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        \n",
    "        # Additional 2 of 3 layers\n",
    "        self.objectDetectorPart2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        \n",
    "        # Additional 3 of 3 layers (for after skip connection)\n",
    "        self.objectDetectorPart3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3072, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "        \n",
    "        # Adding output layer \n",
    "        self.output = nn.Sequential (\n",
    "            nn.Conv2d(in_channels=1024, out_channels=numAnchorBoxes*5, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        )    \n",
    "        \n",
    "       \n",
    "    def forward(self, modelInput):\n",
    "        # Not Executing the MaxPool that is at #52 yet, need to record the skip connection\n",
    "        tmp = self.featureExtractor[0:52](modelInput)   \n",
    "        \n",
    "        # Recording the high resolution features\n",
    "        skipConnection = tmp\n",
    "            \n",
    "        # Concatting the high/low res features (changes from 512*26*26 -> 2048*13*13)\n",
    "        skipConnection = space_to_depth(skipConnection, 2)\n",
    "        \n",
    "        # Executing the MaxPool at #52\n",
    "        tmp = self.featureExtractor[52](tmp)  \n",
    "        \n",
    "        tmp = self.objectDetectorPart1(tmp)\n",
    "        \n",
    "        tmp = self.objectDetectorPart2(tmp)\n",
    "        \n",
    "        tmp = torch.cat([tmp, skipConnection],dim=1)\n",
    "        \n",
    "        tmp = self.objectDetectorPart3(tmp)  \n",
    "        \n",
    "        tmp = self.output(tmp)  \n",
    "        \n",
    "        tmp = tmp.reshape((-1, self.gridSize, self.gridSize, 5*self.numAnchorBoxes))\n",
    "        \n",
    "        return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6028fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model\n",
    "model = DogDetectorModel(gridSize=GRID_SIZE, numAnchorBoxes=NUM_ANCHOR_BOXES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b332292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the model to the GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "395502e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ce801b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 13, 13, 35])\n",
      "torch.Size([2, 13, 13, 35])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, NUM_EPOCHS):\n",
    "#     # ****************** TRAINING ******************\n",
    "#     # Setting the model to training\n",
    "#     model.train()\n",
    "    \n",
    "#     for batchIndex, [modelInput, label] in enumerate(trainDataLoader):\n",
    "#         # Setting the model to training\n",
    "#         model.train()\n",
    "        \n",
    "#         # Moving the model input/label to GPU \n",
    "#         modelInput = modelInput.to(device)\n",
    "#         label = label.to(device)\n",
    "        \n",
    "#          # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         output = model(modelInput)\n",
    "#         print(output.shape)\n",
    "        \n",
    "#         # ****************** VALIDATION ******************\n",
    "        \n",
    "#         break\n",
    "#         print(batchIndex)\n",
    "        \n",
    "    # ****************** VALIDATION ******************\n",
    "    \n",
    "    # Setting the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Turning of the gradient\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Iterating through the batches of the validation data\n",
    "        for batchIndex, [modelInput, label] in enumerate(validationDataLoader):\n",
    "\n",
    "            # Moving the model input/label to GPU \n",
    "            modelInput = modelInput.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            print(model(modelInput).shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            print(label.shape)\n",
    "\n",
    "\n",
    "\n",
    "            break\n",
    "    break\n",
    "        \n",
    "        \n",
    "    \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
