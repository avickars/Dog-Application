{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5993b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import OpenImagesDataset\n",
    "from model_utils import device\n",
    "from model_transformations import Transformations\n",
    "from torch.utils.data import DataLoader\n",
    "from params import NUM_EPOCHS, GRID_SIZE,IMAGE_SIZE, NUM_ANCHOR_BOXES, BATCH_SIZE\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from model_loss import YoloLoss\n",
    "from tqdm import tqdm\n",
    "from model import DogDetectorModel\n",
    "import numpy as np\n",
    "from model_decoder import Decoder\n",
    "from non_max_surpression import NonMaxSurpression\n",
    "from true_box_extractor import true_box_extractor\n",
    "from mean_average_precision import MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b7bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the training data\n",
    "trainingData = OpenImagesDataset(rootDirectory='open-images-v6', \n",
    "                                 anchorBoxes='centroids.npy', \n",
    "                                 transform=Transformations, \n",
    "                                 dataType='train', \n",
    "                                 gridSize=GRID_SIZE, \n",
    "                                 imageSize=IMAGE_SIZE)    \n",
    "\n",
    "# Defining the training data\n",
    "trainDataLoader = DataLoader(dataset=trainingData, \n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=4,\n",
    "                             shuffle=False)\n",
    "\n",
    "# Reading in the training data\n",
    "validationData = OpenImagesDataset(rootDirectory='open-images-v6', \n",
    "                                 anchorBoxes='centroids.npy', \n",
    "                                 transform=Transformations, \n",
    "                                 dataType='validation', \n",
    "                                 gridSize=GRID_SIZE, \n",
    "                                 imageSize=IMAGE_SIZE)    \n",
    "\n",
    "# Defining the training data\n",
    "validationDataLoader = DataLoader(dataset=trainingData, \n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=4,\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab25976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model\n",
    "model = DogDetectorModel(gridSize=13, numAnchorBoxes=NUM_ANCHOR_BOXES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4dd0660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the model to the GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f88e244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the optimizer\n",
    "optimizer = optim.SGD(list(model.parameters()), lr=0.00001, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "977f73e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossYolo = YoloLoss('centroids.npy', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e944142",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInput, label = trainingData.__getitem__(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d16a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInput = modelInput.reshape((1,3,416,416))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d9ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = label.reshape((1,13,13,35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop =  tqdm(enumerate(trainDataLoader), total=len(trainDataLoader), leave=True)\n",
    "# for batchIndex, (modelInput, label) in loop:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115bd217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the non max supressor for predictions\n",
    "nonMaxSurpressionPredicted = NonMaxSurpression()\n",
    "\n",
    "# Initializing the docoder\n",
    "decoder = Decoder('centroids.npy', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, NUM_EPOCHS):    \n",
    "    # ****************** TRAINING ******************\n",
    "    \n",
    "    # Setting the model to training\n",
    "    model.train()\n",
    "    \n",
    "    # Defining list to hold the mean loss\n",
    "    meanLoss = []\n",
    "    \n",
    "    # Defining lists to hold all the predictions and ground truths for all elements in each batch\n",
    "    predictedBoxesAllBatches = []\n",
    "    trueBoxesAllBatches = []\n",
    "    \n",
    "    # Defining loop to get the nice progress bar\n",
    "    loop =  tqdm(enumerate(trainDataLoader), total=len(trainDataLoader), leave=True)\n",
    "    \n",
    "    for batchIndex, (modelInput, label) in loop:\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Moving the model input/label to GPU \n",
    "        modelInput = modelInput.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        # Computing the model output\n",
    "        output = model(modelInput)\n",
    "\n",
    "        # Computing the loss\n",
    "        loss = YoloLoss(output, label)\n",
    "        \n",
    "        # Backpropogating the error\n",
    "        loss.backward()\n",
    "        \n",
    "        # Executing gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        # Appending the loss to the list\n",
    "        meanLoss.append(loss.item())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Decoding the labels\n",
    "            decodedLabel = decoder(torch.clone(label), 'label')\n",
    "            \n",
    "            # Decoding the output\n",
    "            decodedOutput = decoder(torch.clone(output))    \n",
    "            \n",
    "            # Executing non max surpression on the decoded output to get the object detectiosn\n",
    "            predictedBoxes = nonMaxSurpressionPredicted(decodedOutput)\n",
    "            \n",
    "            # Extracting the true bound boxes from the decoded label\n",
    "            trueBoxes = true_box_extractor(label, decodedLabel)\n",
    "            \n",
    "            # Adding the detected boxes onto the list\n",
    "            predictedBoxesAllBatches += predictedBoxes\n",
    "            \n",
    "            # Adding the true boxes onto the list\n",
    "            trueBoxesAllBatches += trueBoxes\n",
    "    \n",
    "    # Updating the output\n",
    "    loop.set_description(f\"Epoch: [{epoch+1}/{NUM_EPOCHS}]\")     \n",
    "    loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        \n",
    "    # Computing the MAP value\n",
    "    mapValue = MAP(predictedBoxesAllBatches.copy(), trueBoxesAllBatches.copy())    \n",
    "        \n",
    "    print(f\"====> Train Mean Loss: {sum(meanLoss)/len(meanLoss)}\")\n",
    "    print(f\"====> Train MAP: {mapValue}\")\n",
    "                \n",
    "    # ****************** VALIDATION ******************\n",
    "    \n",
    "    # Setting the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Resetting lists from above\n",
    "    meanLoss = []\n",
    "    predictedBoxesAllBatches = []\n",
    "    trueBoxesAllBatches = []\n",
    "    \n",
    "    # Turning of the gradient\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Defining loop to get the nice progress bar for the validation data\n",
    "        loop =  tqdm(enumerate(validationDataLoader), total=len(validationDataLoader), leave=True)\n",
    "        \n",
    "        # Iterating through the batches of the validation data\n",
    "        for batchIndex, [modelInput, label] in loop:\n",
    "\n",
    "            # Moving the model input/label to GPU \n",
    "            modelInput = modelInput.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Computing the model output\n",
    "            output = model(modelInput)\n",
    "            \n",
    "            # Computing the loss\n",
    "            loss = YoloLoss(output, label)\n",
    "            \n",
    "            # Appending the loss\n",
    "            meanLoss.append(loss.item())\n",
    "            \n",
    "            # Decoding the labels\n",
    "            decodedLabel = decoder(torch.clone(label), 'label')\n",
    "            \n",
    "            # Decoding the output\n",
    "            decodedOutput = decoder(torch.clone(output))    \n",
    "            \n",
    "            # Executing non max surpression on the decoded output to get the object detectiosn\n",
    "            predictedBoxes = nonMaxSurpressionPredicted(decodedOutput)\n",
    "            \n",
    "            # Extracting the true bound boxes from the decoded label\n",
    "            trueBoxes = true_box_extractor(label, decodedLabel)\n",
    "            \n",
    "            # Adding the detected boxes onto the list\n",
    "            predictedBoxesAllBatches += predictedBoxes\n",
    "            \n",
    "            # Adding the true boxes onto the list\n",
    "            trueBoxesAllBatches += trueBoxes\n",
    "            \n",
    "        loop.set_description(f\"Epoch: [{epoch+1}/{NUM_EPOCHS}]\")     \n",
    "        loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "        # Computing the MAP value\n",
    "        mapValue = MAP(predictedBoxesAllBatches.copy(), trueBoxesAllBatches.copy())    \n",
    "            \n",
    "        print(f\"====> Validation Loss: {sum(meanLoss)/len(meanLoss)}\")\n",
    "        print(f\"====> Validation MAP: {mapValue}\")\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import plot_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dbb9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tensor(modelInput[0].cpu(), decodedOutput[0][ 5,  7, 20:25].reshape((1,-1)).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
